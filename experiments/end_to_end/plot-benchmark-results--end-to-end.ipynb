{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import statistics\n",
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_csv_results():\n",
    "    normal_scenarios = [\"feature_importance\", \"operator_impact\", \"data_corruption\", \"data_cleaning\"]\n",
    "    image_scenarios = [\"data_corruption\", \"data_cleaning\"]\n",
    "    normal_datasets = [\"healthcare\", \"folktables\", \"cardio\", \"reviews\"]\n",
    "    image_dataset = [\"sneakers\"]\n",
    "    data_loadings = [\"fast_loading\", \"slow_loading\"]\n",
    "    featurizations = [\"featurization_0\", \"featurization_1\", \"featurization_2\", \"featurization_3\", \"featurization_4\"]\n",
    "    models = [\"logistic_regression\", \"xgboost\", \"neural_network\"]\n",
    "    result_df = None\n",
    "    for scenario in normal_scenarios:\n",
    "        for dataset in normal_datasets:\n",
    "            for data_loading in data_loadings:\n",
    "                for featurization in featurizations:\n",
    "                    for model in models:\n",
    "                        filepath = f\"{os.getcwd()}/end-to-end-benchmark-results/\" \\\n",
    "                                   f\"results-{scenario}-{dataset}-{data_loading}-{featurization}-{model}.csv\"\n",
    "                        new_df = pd.read_csv(filepath)\n",
    "                        new_df['median_no_opt'] = new_df['total_exec_duration_without_opt'].median()\n",
    "                        new_df['median_opt'] = new_df['total_exec_duration_with_opt'].median()\n",
    "                        new_df['median_speedup'] = new_df['median_no_opt'] / new_df['median_opt']\n",
    "                        new_df['opt_original_pipeline_estimated'] = new_df['opt_original_pipeline_estimated'].median()\n",
    "\n",
    "                        new_df['median_model_training_opt'] = new_df['opt_original_pipeline_model_training'].median() \\\n",
    "                                                              + new_df['opt_what_if_execution_combined_model_training'].median()\n",
    "                        new_df['median_model_training_no_opt'] = new_df['opt_original_pipeline_model_training'].median() \\\n",
    "                                                              + new_df['no_opt_what_if_execution_combined_model_training'].median()\n",
    "                        new_df['median_opt_wo_training'] = new_df['median_opt'] - new_df['median_model_training_opt']\n",
    "                        new_df['median_no_opt_wo_training'] = new_df['median_no_opt'] - \\\n",
    "                                                              new_df['median_model_training_no_opt']\n",
    "                        new_df['median_speedup_excluding_model_training'] = new_df['median_no_opt_wo_training'] / \\\n",
    "                                                                            new_df['median_opt_wo_training']\n",
    "\n",
    "                        # for column in new_df.columns:\n",
    "                        #     if column.startswith(\"analysis_result\"):\n",
    "                        #         new_df[column] = median_no_opt / new_df[column].median()\n",
    "                        new_df['variant_count_including_orig'] = new_df['variant_count'] + 1\n",
    "                        new_df['max_possible_speedup'] = new_df['variant_count_including_orig'] / 2\n",
    "                        new_df = new_df[['median_no_opt', 'median_opt', 'median_speedup',\n",
    "                                         'median_speedup_excluding_model_training',\n",
    "                                         'opt_original_pipeline_estimated', 'variant_count_including_orig',\n",
    "                                         'max_possible_speedup', 'scenario', 'dataset',\n",
    "                                         'data_loading', 'featurization', 'model']]\n",
    "                        new_df = new_df.head(1)\n",
    "                        new_df = new_df.round(2)\n",
    "                        if result_df is None:\n",
    "                            result_df = new_df\n",
    "                        else:\n",
    "                            result_df = pd.concat([result_df, new_df], axis=0)\n",
    "    for scenario in image_scenarios:\n",
    "        for dataset in image_dataset:\n",
    "            for data_loading in data_loadings:\n",
    "                for featurization in [\"image\"]:\n",
    "                    for model in [\"image\"]:\n",
    "                        filepath = f\"{os.getcwd()}/end-to-end-benchmark-results/\" \\\n",
    "                                   f\"results-{scenario}-{dataset}-{data_loading}-{featurization}-{model}.csv\"\n",
    "                        new_df = pd.read_csv(filepath)\n",
    "                        median_no_opt = new_df['total_exec_duration_without_opt'].median()\n",
    "                        new_df['median_no_opt'] = median_no_opt\n",
    "                        median_opt = new_df['total_exec_duration_with_opt'].median()\n",
    "                        new_df['median_opt'] = median_opt\n",
    "                        median_speedup = median_no_opt / median_opt\n",
    "                        new_df['median_speedup'] = median_speedup\n",
    "                        new_df['opt_original_pipeline_estimated'] = new_df['opt_original_pipeline_estimated'].median()\n",
    "\n",
    "                        new_df['median_model_training_opt'] = new_df['opt_original_pipeline_model_training'].median() \\\n",
    "                                                              + new_df['opt_what_if_execution_combined_model_training'].median()\n",
    "                        new_df['median_model_training_no_opt'] = new_df['opt_original_pipeline_model_training'].median() \\\n",
    "                                                              + new_df['no_opt_what_if_execution_combined_model_training'].median()\n",
    "                        new_df['median_opt_wo_training'] = new_df['median_opt'] - new_df['median_model_training_opt']\n",
    "                        new_df['median_no_opt_wo_training'] = new_df['median_no_opt'] - \\\n",
    "                                                              new_df['median_model_training_no_opt']\n",
    "                        new_df['median_speedup_excluding_model_training'] = new_df['median_no_opt_wo_training'] / \\\n",
    "                                                                            new_df['median_opt_wo_training']\n",
    "                        # for column in new_df.columns:\n",
    "                        #     if column.startswith(\"analysis_result\"):\n",
    "                        #         new_df[column] = median_no_opt / new_df[column].median()\n",
    "                        new_df['variant_count_including_orig'] = new_df['variant_count'] + 1\n",
    "                        new_df['max_possible_speedup'] = new_df['variant_count_including_orig'] / 2\n",
    "                        new_df = new_df[['median_no_opt', 'median_opt', 'median_speedup',\n",
    "                                         'median_speedup_excluding_model_training',\n",
    "                                         'opt_original_pipeline_estimated', 'variant_count_including_orig',\n",
    "                                         'max_possible_speedup', 'scenario', 'dataset',\n",
    "                                         'data_loading', 'featurization', 'model']]\n",
    "                        new_df = new_df.head(1)\n",
    "                        new_df = new_df.round(2)\n",
    "                        if result_df is None:\n",
    "                            result_df = new_df\n",
    "                        else:\n",
    "                            result_df = pd.concat([result_df, new_df], axis=0)\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "median_results = load_csv_results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "    median_no_opt  median_opt  median_speedup  \\\n0         1062.38     1071.82            0.99   \n0         1966.87     1969.85            1.00   \n0        10687.28    10733.35            1.00   \n0         3146.19     3121.97            1.01   \n0         7434.22     7336.14            1.01   \n..            ...         ...             ...   \n0        63574.28     4416.71           14.39   \n0        65978.98     4426.04           14.91   \n0        56376.38     3736.61           15.09   \n0        74809.10     4705.74           15.90   \n0        71200.11     4379.44           16.26   \n\n    median_speedup_excluding_model_training  opt_original_pipeline_estimated  \\\n0                                      0.97                           400.72   \n0                                      0.99                           844.37   \n0                                      0.98                          5255.19   \n0                                      1.02                          1306.48   \n0                                      1.04                          3358.81   \n..                                      ...                              ...   \n0                                     13.65                          1939.64   \n0                                     13.86                          1957.33   \n0                                      4.67                          1163.73   \n0                                     13.50                          1979.33   \n0                                     14.68                          1924.09   \n\n    variant_count_including_orig  max_possible_speedup         scenario  \\\n0                              2                   1.0  operator_impact   \n0                              2                   1.0  operator_impact   \n0                              2                   1.0  operator_impact   \n0                              2                   1.0  operator_impact   \n0                              2                   1.0  operator_impact   \n..                           ...                   ...              ...   \n0                             31                  15.5  data_corruption   \n0                             31                  15.5  data_corruption   \n0                             34                  17.0  data_corruption   \n0                             31                  15.5  data_corruption   \n0                             31                  15.5  data_corruption   \n\n       dataset  data_loading    featurization                model  \n0       cardio  fast_loading  featurization_0  logistic_regression  \n0       cardio  fast_loading  featurization_0              xgboost  \n0       cardio  fast_loading  featurization_0       neural_network  \n0   folktables  fast_loading  featurization_0  logistic_regression  \n0   folktables  fast_loading  featurization_0       neural_network  \n..         ...           ...              ...                  ...  \n0   healthcare  fast_loading  featurization_1              xgboost  \n0   healthcare  slow_loading  featurization_2              xgboost  \n0       cardio  fast_loading  featurization_3              xgboost  \n0   healthcare  fast_loading  featurization_3              xgboost  \n0   healthcare  slow_loading  featurization_1              xgboost  \n\n[484 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>median_no_opt</th>\n      <th>median_opt</th>\n      <th>median_speedup</th>\n      <th>median_speedup_excluding_model_training</th>\n      <th>opt_original_pipeline_estimated</th>\n      <th>variant_count_including_orig</th>\n      <th>max_possible_speedup</th>\n      <th>scenario</th>\n      <th>dataset</th>\n      <th>data_loading</th>\n      <th>featurization</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1062.38</td>\n      <td>1071.82</td>\n      <td>0.99</td>\n      <td>0.97</td>\n      <td>400.72</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>operator_impact</td>\n      <td>cardio</td>\n      <td>fast_loading</td>\n      <td>featurization_0</td>\n      <td>logistic_regression</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1966.87</td>\n      <td>1969.85</td>\n      <td>1.00</td>\n      <td>0.99</td>\n      <td>844.37</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>operator_impact</td>\n      <td>cardio</td>\n      <td>fast_loading</td>\n      <td>featurization_0</td>\n      <td>xgboost</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>10687.28</td>\n      <td>10733.35</td>\n      <td>1.00</td>\n      <td>0.98</td>\n      <td>5255.19</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>operator_impact</td>\n      <td>cardio</td>\n      <td>fast_loading</td>\n      <td>featurization_0</td>\n      <td>neural_network</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>3146.19</td>\n      <td>3121.97</td>\n      <td>1.01</td>\n      <td>1.02</td>\n      <td>1306.48</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>operator_impact</td>\n      <td>folktables</td>\n      <td>fast_loading</td>\n      <td>featurization_0</td>\n      <td>logistic_regression</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>7434.22</td>\n      <td>7336.14</td>\n      <td>1.01</td>\n      <td>1.04</td>\n      <td>3358.81</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>operator_impact</td>\n      <td>folktables</td>\n      <td>fast_loading</td>\n      <td>featurization_0</td>\n      <td>neural_network</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>63574.28</td>\n      <td>4416.71</td>\n      <td>14.39</td>\n      <td>13.65</td>\n      <td>1939.64</td>\n      <td>31</td>\n      <td>15.5</td>\n      <td>data_corruption</td>\n      <td>healthcare</td>\n      <td>fast_loading</td>\n      <td>featurization_1</td>\n      <td>xgboost</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>65978.98</td>\n      <td>4426.04</td>\n      <td>14.91</td>\n      <td>13.86</td>\n      <td>1957.33</td>\n      <td>31</td>\n      <td>15.5</td>\n      <td>data_corruption</td>\n      <td>healthcare</td>\n      <td>slow_loading</td>\n      <td>featurization_2</td>\n      <td>xgboost</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>56376.38</td>\n      <td>3736.61</td>\n      <td>15.09</td>\n      <td>4.67</td>\n      <td>1163.73</td>\n      <td>34</td>\n      <td>17.0</td>\n      <td>data_corruption</td>\n      <td>cardio</td>\n      <td>fast_loading</td>\n      <td>featurization_3</td>\n      <td>xgboost</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>74809.10</td>\n      <td>4705.74</td>\n      <td>15.90</td>\n      <td>13.50</td>\n      <td>1979.33</td>\n      <td>31</td>\n      <td>15.5</td>\n      <td>data_corruption</td>\n      <td>healthcare</td>\n      <td>fast_loading</td>\n      <td>featurization_3</td>\n      <td>xgboost</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>71200.11</td>\n      <td>4379.44</td>\n      <td>16.26</td>\n      <td>14.68</td>\n      <td>1924.09</td>\n      <td>31</td>\n      <td>15.5</td>\n      <td>data_corruption</td>\n      <td>healthcare</td>\n      <td>slow_loading</td>\n      <td>featurization_1</td>\n      <td>xgboost</td>\n    </tr>\n  </tbody>\n</table>\n<p>484 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_results = median_results.sort_values(by=['median_speedup'])\n",
    "median_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "median_results.to_csv(f\"{os.getcwd()}/end-to-end-benchmark-results/end_to_end_speedup_overview.csv\", index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}